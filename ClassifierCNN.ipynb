{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aecec99c-7ae2-4224-a506-dd8fb99c9eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d8005a-d577-4bea-8d3d-6aa34cc0bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForgeryClassificationDataset(Dataset):\n",
    "    def __init__(self, authentic_dir, forged_dir, img_size=128):\n",
    "        self.img_paths = []\n",
    "        self.labels = []\n",
    "        self.size = img_size\n",
    "\n",
    "        # authentic = label 0\n",
    "        for p in sorted(glob.glob(os.path.join(authentic_dir, \"*.png\"))):\n",
    "            self.img_paths.append(p)\n",
    "            self.labels.append(0)\n",
    "\n",
    "        # forged = label 1\n",
    "        for p in sorted(glob.glob(os.path.join(forged_dir, \"*.png\"))):\n",
    "            self.img_paths.append(p)\n",
    "            self.labels.append(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # load + resize\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((self.size, self.size))\n",
    "        img_np = np.array(img).astype(np.float32) / 255.0\n",
    "\n",
    "        # to CHW tensor\n",
    "        img_t = torch.tensor(img_np).permute(2, 0, 1)\n",
    "        label_t = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return img_t, label_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3100159-3eed-4812-b5c9-a2c9d3ee0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Baseline CNN classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),       # 64x64\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),       # 32x32\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),       # 16x16\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 16 * 16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2),      # authentic vs forged\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbea12bc-1da7-4299-8e39-a9fb42808795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier():\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    ds = ForgeryClassificationDataset(\n",
    "        authentic_dir=\"data/train_images/authentic\",\n",
    "        forged_dir=\"data/train_images/forged\",\n",
    "        img_size=128\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(ds, batch_size=16, shuffle=True, num_workers=0)\n",
    "\n",
    "    model = ClassifierCNN().to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        acc = correct / len(ds)\n",
    "        print(f\"Epoch {epoch+1} | loss={total_loss/len(loader):.4f} | acc={acc:.3f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"baseline_classifier.pth\")\n",
    "    print(\"Saved baseline classifier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec9b4bc-be3e-49ca-bf78-a9fba34be384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1 | loss=1165.4872 | acc=0.512\n",
      "Epoch 2 | loss=0.7012 | acc=0.513\n",
      "Epoch 3 | loss=0.6976 | acc=0.509\n",
      "Epoch 4 | loss=0.6964 | acc=0.529\n",
      "Epoch 5 | loss=0.6999 | acc=0.504\n",
      "Epoch 6 | loss=0.6943 | acc=0.520\n",
      "Epoch 7 | loss=0.6982 | acc=0.513\n",
      "Epoch 8 | loss=0.6977 | acc=0.516\n",
      "Epoch 9 | loss=0.6952 | acc=0.526\n",
      "Epoch 10 | loss=0.6988 | acc=0.507\n",
      "Saved baseline classifier.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29986746-6efe-4f78-8293-76e58bfa58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierCNN_update(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 128x128 -> 64x64 (example input)\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 64x64 -> 32x32\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, dilation=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, dilation=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 32x32 -> 16x16\n",
    "\n",
    "            # Block 4 (optional, for capturing larger context)\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=2, dilation=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=2, dilation=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)  # 16x16 -> 8x8\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),  # output 256x1x1\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 2)  # authentic vs forged\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87087584-5bc2-48eb-aa7e-098f67103c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_improved():\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    ds = ForgeryClassificationDataset(\n",
    "        authentic_dir=\"data/train_images/authentic\",\n",
    "        forged_dir=\"data/train_images/forged\",\n",
    "        img_size=128\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(ds, batch_size=16, shuffle=True, num_workers=0)\n",
    "\n",
    "    model = ClassifierCNN_update().to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        acc = correct / len(ds)\n",
    "        print(f\"Epoch {epoch+1} | loss={total_loss/len(loader):.4f} | acc={acc:.3f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"baseline_classifier.pth\")\n",
    "    print(\"Saved baseline classifier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c82dd38b-ce81-496b-93a3-3f32bb8b13a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1 | loss=0.9028 | acc=0.514\n",
      "Epoch 2 | loss=0.6952 | acc=0.520\n",
      "Epoch 3 | loss=0.6933 | acc=0.521\n",
      "Epoch 4 | loss=0.6988 | acc=0.519\n",
      "Epoch 5 | loss=0.6951 | acc=0.527\n",
      "Epoch 6 | loss=0.6958 | acc=0.513\n",
      "Epoch 7 | loss=0.6977 | acc=0.524\n",
      "Epoch 8 | loss=0.6973 | acc=0.519\n",
      "Epoch 9 | loss=0.6952 | acc=0.526\n",
      "Epoch 10 | loss=0.6956 | acc=0.517\n",
      "Saved baseline classifier.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_classifier_improved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc2a0591-2723-4c27-a7bf-8ff4b9229324",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Small CNN to extract features from each patch.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, feature_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        self.feature_dim = feature_dim\n",
    "        self.fc = nn.Linear(64, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, 3, patch_h, patch_w)\n",
    "        x = self.conv(x)          # (batch, 64, 1, 1)\n",
    "        x = x.view(x.size(0), -1) # (batch, 64)\n",
    "        x = self.fc(x)            # (batch, feature_dim)\n",
    "        x = F.normalize(x, dim=1) # normalize for similarity comparisons\n",
    "        return x\n",
    "\n",
    "class PatchBasedCopyMoveDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Detects copy-move forgery by comparing patch features.\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size=32, stride=16, feature_dim=64):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.feature_extractor = PatchFeatureExtractor(feature_dim=feature_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)  # authentic vs forged\n",
    "        )\n",
    "\n",
    "    def extract_patches(self, x):\n",
    "        \"\"\"\n",
    "        Extracts overlapping patches from input image.\n",
    "        x: (batch, 3, H, W)\n",
    "        returns: patches (num_patches_total, 3, patch_h, patch_w)\n",
    "        \"\"\"\n",
    "        patches = x.unfold(2, self.patch_size, self.stride) \\\n",
    "                   .unfold(3, self.patch_size, self.stride)\n",
    "        # patches: (batch, 3, num_h, num_w, patch_h, patch_w)\n",
    "        batch, c, num_h, num_w, ph, pw = patches.shape\n",
    "        patches = patches.permute(0,2,3,1,4,5).contiguous()\n",
    "        patches = patches.view(-1, c, ph, pw)\n",
    "        return patches, num_h, num_w\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract patches\n",
    "        patches, num_h, num_w = self.extract_patches(x)\n",
    "\n",
    "        # Extract features for each patch\n",
    "        patch_features = self.feature_extractor(patches)  # (num_patches_total, feature_dim)\n",
    "\n",
    "        # Compute similarity map: naive example using mean features\n",
    "        patch_features = patch_features.view(x.size(0), num_h*num_w, -1)\n",
    "        mean_features = patch_features.mean(dim=1)  # (batch, feature_dim)\n",
    "\n",
    "        # Classify based on aggregated patch features\n",
    "        out = self.classifier(mean_features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e430486e-6a7b-4599-91aa-dc865f63ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_patch():\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    ds = ForgeryClassificationDataset(\n",
    "        authentic_dir=\"data/train_images/authentic\",\n",
    "        forged_dir=\"data/train_images/forged\",\n",
    "        img_size=128\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(ds, batch_size=16, shuffle=True, num_workers=0)\n",
    "\n",
    "    model = PatchBasedCopyMoveDetector().to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        acc = correct / len(ds)\n",
    "        print(f\"Epoch {epoch+1} | loss={total_loss/len(loader):.4f} | acc={acc:.3f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"baseline_classifier.pth\")\n",
    "    print(\"Saved baseline classifier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1628451a-87d2-4709-a1ae-8426853a5c2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1 | loss=0.6971 | acc=0.520\n",
      "Epoch 2 | loss=0.6951 | acc=0.518\n",
      "Epoch 3 | loss=0.6959 | acc=0.517\n",
      "Epoch 4 | loss=0.6951 | acc=0.515\n",
      "Epoch 5 | loss=0.6972 | acc=0.508\n",
      "Epoch 6 | loss=0.6969 | acc=0.517\n",
      "Epoch 7 | loss=0.6989 | acc=0.512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     train_classifier_patch()\n",
      "Cell \u001b[1;32mIn[17], line 23\u001b[0m, in \u001b[0;36mtrain_classifier_patch\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     21\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     24\u001b[0m     imgs, labels \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     26\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(imgs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    738\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m, in \u001b[0;36mForgeryClassificationDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     22\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# load + resize\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize))\n\u001b[0;32m     26\u001b[0m img_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# to CHW tensor\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\PIL\\Image.py:2304\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2292\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2293\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2294\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[0;32m   2295\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2296\u001b[0m         )\n\u001b[0;32m   2297\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2298\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2299\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2300\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2301\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2302\u001b[0m         )\n\u001b[1;32m-> 2304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim\u001b[38;5;241m.\u001b[39mresize(size, resample, box))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_classifier_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0a5468f-a737-465d-8045-08555e3a24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Small CNN producing a feature vector for each patch.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "        )\n",
    "        self.fc = nn.Linear(64, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)   # (batch, 64)\n",
    "        x = self.fc(x)\n",
    "        x = F.normalize(x, dim=1)   # important for cosine similarity\n",
    "        return x\n",
    "\n",
    "# ---------------------------\n",
    "# Full Copy-Move Model\n",
    "# ---------------------------\n",
    "class CopyMoveSelfSimilarity(nn.Module):\n",
    "    \"\"\"\n",
    "    Patch-based model using self-similarity matrix for copy-move detection.\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size=32, stride=16, feature_dim=64):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        self.patch_net = PatchFeatureExtractor(feature_dim)\n",
    "\n",
    "        # Classifier takes summary of similarity matrix\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)   # authentic vs forged\n",
    "        )\n",
    "\n",
    "    # ---------------------------\n",
    "    # Patch extraction\n",
    "    # ---------------------------\n",
    "    def extract_patches(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, 3, H, W)\n",
    "        returns:\n",
    "            patches: (B * num_patches, 3, P, P)\n",
    "            num_h, num_w: patch grid size\n",
    "        \"\"\"\n",
    "        patches = x.unfold(2, self.patch_size, self.stride) \\\n",
    "                   .unfold(3, self.patch_size, self.stride)\n",
    "        \n",
    "        B, C, num_h, num_w, P1, P2 = patches.shape\n",
    "        \n",
    "        patches = patches.permute(0,2,3,1,4,5).contiguous()\n",
    "        patches = patches.view(-1, C, P1, P2)\n",
    "\n",
    "        return patches, num_h, num_w\n",
    "\n",
    "    # ---------------------------\n",
    "    # Forward pass\n",
    "    # ---------------------------\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, 3, H, W)\n",
    "        \"\"\"\n",
    "        B = x.size(0)\n",
    "\n",
    "        # 1. Extract patches\n",
    "        patches, num_h, num_w = self.extract_patches(x)\n",
    "        N = num_h * num_w   # patches per image\n",
    "\n",
    "        # 2. Get features for each patch\n",
    "        feat = self.patch_net(patches)    # (B*N, feature_dim)\n",
    "\n",
    "        # 3. Reshape back per image\n",
    "        feat = feat.view(B, N, self.feature_dim)  # (B, N, F)\n",
    "\n",
    "        # 4. Compute self-similarity matrix for each image\n",
    "        #    sim[b] = N x N matrix of cosine similarities\n",
    "        sim = torch.bmm(feat, feat.transpose(1, 2))   # (B, N, N)\n",
    "\n",
    "        # 5. Self-similarity statistics (summary)\n",
    "        #    These patterns differ strongly between authentic and forged images\n",
    "        max_sim = sim.max(dim=2).values.mean(dim=1)   # (B,)\n",
    "        mean_sim = sim.mean(dim=(1,2))                # (B,)\n",
    "        var_sim = sim.var(dim=(1,2))                  # (B,)\n",
    "        topk_sim = torch.topk(sim.view(B, -1), 5, dim=1).values.mean(dim=1)\n",
    "\n",
    "        # feature vector: [mean, max, variance, avg_top5]\n",
    "        stats = torch.stack([mean_sim, max_sim, var_sim, topk_sim], dim=1)\n",
    "\n",
    "        # 6. Classify (authentic vs forged)\n",
    "        out = self.classifier(stats)\n",
    "\n",
    "        return out   # also return similarity matrix for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b7aa799-d57d-4234-a488-663aa2018ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_patch_2():\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    ds = ForgeryClassificationDataset(\n",
    "        authentic_dir=\"data/train_images/authentic\",\n",
    "        forged_dir=\"data/train_images/forged\",\n",
    "        img_size=128\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(ds, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "    model = CopyMoveSelfSimilarity().to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        acc = correct / len(ds)\n",
    "        print(f\"Epoch {epoch+1} | loss={total_loss/len(loader):.4f} | acc={acc:.3f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"baseline_classifier.pth\")\n",
    "    print(\"Saved baseline classifier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0b90d58-2404-46f6-b7c0-42688174341c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1 | loss=0.6956 | acc=0.519\n",
      "Epoch 2 | loss=0.6959 | acc=0.517\n",
      "Epoch 3 | loss=0.6975 | acc=0.514\n",
      "Epoch 4 | loss=0.6952 | acc=0.527\n",
      "Epoch 5 | loss=0.6970 | acc=0.514\n",
      "Epoch 6 | loss=0.6963 | acc=0.527\n",
      "Epoch 7 | loss=0.6969 | acc=0.517\n",
      "Epoch 8 | loss=0.6953 | acc=0.521\n",
      "Epoch 9 | loss=0.6955 | acc=0.519\n",
      "Epoch 10 | loss=0.6972 | acc=0.513\n",
      "Saved baseline classifier.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_classifier_patch_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5f32b-6d8b-4b2d-97be-322bda7e290a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
