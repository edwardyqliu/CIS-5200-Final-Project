{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define config and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import torch\n",
    "import json\n",
    "import math\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from skimage import exposure, restoration\n",
    "from skimage.feature import local_binary_pattern\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Configuration Class (Enhanced)\n",
    "\n",
    "class CFG:\n",
    "    # Paths\n",
    "    train_images_path = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_images\"\n",
    "    test_images_path = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/test_images\"\n",
    "    train_masks_path = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/train_masks\"\n",
    "    sample_sub_path = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection/sample_submission.csv\"\n",
    "    \n",
    "    # Model paths\n",
    "    dino_path = \"/kaggle/input/dinov2/pytorch/base/1\"\n",
    "    dino_weights_path = \"/kaggle/input/m/ravaghi/dinov2/pytorch/base/1/model.pt\"\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Image processing\n",
    "    img_size = 512\n",
    "    batch_size = 4\n",
    "    \n",
    "    # Inference settings\n",
    "    use_tta = True\n",
    "    tta_flip_horizontal = True\n",
    "    tta_flip_vertical = True\n",
    "    \n",
    "    # Post-processing hyperparameters (to be tuned)\n",
    "    alpha_grad = 0.35\n",
    "    threshold_multiplier = 0.3\n",
    "    morph_close_kernel = 5\n",
    "    morph_open_kernel = 3\n",
    "    \n",
    "    # Hyperparameter tuning ranges\n",
    "    alpha_grad_range = [0.2, 0.3, 0.35, 0.4, 0.5]\n",
    "    threshold_range = [0.2, 0.3, 0.4, 0.5]\n",
    "    kernel_range = [3, 5, 7]\n",
    "    \n",
    "    # EDA settings\n",
    "    eda_sample_size = 100\n",
    "    plot_dpi = 100\n",
    "    \n",
    "    # Random seed for reproducibility\n",
    "    seed = 42\n",
    "    \n",
    "    @staticmethod\n",
    "    def set_seed():\n",
    "        random.seed(CFG.seed)\n",
    "        np.random.seed(CFG.seed)\n",
    "        torch.manual_seed(CFG.seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(CFG.seed)\n",
    "\n",
    "CFG.set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture (Pretrained dino encoder + small decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoTinyDecoder(nn.Module):\n",
    "    def __init__(self, in_ch=768, out_ch=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 256, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(256, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, out_ch, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, f, size):\n",
    "        return self.net(F.interpolate(f, size=size, mode=\"bilinear\", align_corners=False))\n",
    "\n",
    "\n",
    "class DinoSegmenter(nn.Module):\n",
    "    def __init__(self, encoder, processor):\n",
    "        super().__init__()\n",
    "        self.encoder, self.processor = encoder, processor\n",
    "        \n",
    "        for p in self.encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        self.seg_head = DinoTinyDecoder(768, 1)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        imgs = (x*255).clamp(0, 255).byte().permute(0, 2, 3, 1).cpu().numpy()\n",
    "        inputs = self.processor(images=list(imgs), return_tensors=\"pt\").to(x.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            feats = self.encoder(**inputs).last_hidden_state\n",
    "        \n",
    "        B, N, C = feats.shape\n",
    "        fmap = feats[:, 1:, :].permute(0, 2, 1)\n",
    "        s = int(math.sqrt(N-1))\n",
    "        fmap = fmap.reshape(B, C, s, s)\n",
    "        \n",
    "        return fmap\n",
    "\n",
    "    def forward_seg(self, x):\n",
    "        fmap = self.forward_features(x)\n",
    "        return self.seg_head(fmap, (CFG.img_size, CFG.img_size))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define eval and training\n",
    "\n",
    "Dice loss + BCE loss to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        intersection = (pred * target).sum(dim=(2,3))\n",
    "        union = pred.sum(dim=(2,3)) + target.sum(dim=(2,3))\n",
    "        dice = (2 * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_decoder(model, val_loader):\n",
    "    model.eval()\n",
    "    ious = []\n",
    "\n",
    "    for images, masks in val_loader:\n",
    "        images = images.to(CFG.device)\n",
    "        masks = masks.to(CFG.device)\n",
    "\n",
    "        logits = model.forward_seg(images)\n",
    "        preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "\n",
    "        inter = (preds * masks).sum(dim=(2,3))\n",
    "        union = ((preds + masks) > 0).float().sum(dim=(2,3))\n",
    "        iou = ((inter + 1e-6) / (union + 1e-6)).mean().item()\n",
    "\n",
    "        ious.append(iou)\n",
    "\n",
    "    model.train()\n",
    "    return sum(ious) / len(ious)\n",
    "\n",
    "def train_decoder(model, train_loader, val_loader=None, \n",
    "                  epochs=10, lr=1e-4, save_path=\"decoder_trained.pth\",\n",
    "                  use_amp=True, dice_weight=0.3):\n",
    "\n",
    "    device = CFG.device\n",
    "\n",
    "    # Train ONLY the decoder\n",
    "    for p in model.encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.seg_head.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    criterion_bce = nn.BCEWithLogitsLoss()\n",
    "    criterion_dice = DiceLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.seg_head.parameters(), lr=lr)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for images, masks in pbar:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                logits = model.forward_seg(images)  # (B,1,H,W)\n",
    "\n",
    "                loss_bce = criterion_bce(logits, masks)\n",
    "                loss_dice = criterion_dice(logits, masks)\n",
    "\n",
    "                loss = (1 - dice_weight) * loss_bce + dice_weight * loss_dice\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Optional validation\n",
    "        # if val_loader is not None:\n",
    "        #     val_iou = eval_decoder(model, val_loader)\n",
    "        #     print(f\"Val IoU: {val_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loaders and data\n",
    "\n",
    "90 / 10 Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyMoveDataset(Dataset):\n",
    "    \"\"\"\n",
    "    - authentic → zero mask\n",
    "    - forged → load 0/1 mask from .npy file\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_paths, mask_paths, img_size):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths   # contains None for authentic\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        mask_path  = self.mask_paths[idx]\n",
    "\n",
    "        # --- Load and preprocess image ---\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = image.resize((self.img_size, self.img_size))\n",
    "        image = np.array(image, np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "\n",
    "        # --- Load mask ---\n",
    "        if mask_path is None:\n",
    "            # authentic → no mask → zero mask\n",
    "            mask = np.zeros((self.img_size, self.img_size), dtype=np.uint8)\n",
    "\n",
    "        else:\n",
    "            # forged → load .npy binary mask\n",
    "            mask = np.load(mask_path)    # shape is (H, W) or (C, H, W)\n",
    "            \n",
    "            # If mask has channels, collapse\n",
    "            if mask.ndim == 3:\n",
    "                mask = np.max(mask, axis=0)\n",
    "\n",
    "            # Ensure binary\n",
    "            mask = (mask > 0).astype(np.uint8)\n",
    "\n",
    "            # Resize to training size\n",
    "            mask = cv2.resize(mask, (self.img_size, self.img_size), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        mask = torch.from_numpy(mask)[None].float()\n",
    "        return image, mask\n",
    "\n",
    "def get_train_val_loaders(val_ratio=0.1):\n",
    "    image_dir = Path(CFG.train_images_path)\n",
    "    mask_dir  = Path(CFG.train_masks_path)\n",
    "\n",
    "    # --- Load authentic images ---\n",
    "    authentic_images = sorted(list((image_dir / \"authentic\").glob(\"*.*\")))\n",
    "    authentic_dict = {img.stem: img for img in authentic_images}\n",
    "\n",
    "    # --- Load forged images ---\n",
    "    forged_images = sorted(list((image_dir / \"forged\").glob(\"*.*\")))\n",
    "    forged_dict = {img.stem: img for img in forged_images}\n",
    "\n",
    "    # Check masks exist\n",
    "    for img in forged_images:\n",
    "        mpath = mask_dir / (img.stem + \".npy\")\n",
    "        if not mpath.exists():\n",
    "            raise FileNotFoundError(f\"Missing .npy mask for forged image: {img.name}\")\n",
    "\n",
    "    paired_groups = []   # each item: list of (image_path, mask_path)\n",
    "    singletons = []      # same format\n",
    "\n",
    "    # --- Build pairs and singletons ---\n",
    "    all_keys = set(authentic_dict.keys()) | set(forged_dict.keys())\n",
    "\n",
    "    for stem in all_keys:\n",
    "        has_auth = stem in authentic_dict\n",
    "        has_forg = stem in forged_dict\n",
    "\n",
    "        if has_auth and has_forg:\n",
    "            # A matched pair\n",
    "            authentic_img = authentic_dict[stem]\n",
    "            forged_img = forged_dict[stem]\n",
    "            forged_mask = mask_dir / (stem + \".npy\")\n",
    "\n",
    "            paired_groups.append([\n",
    "                (authentic_img, None),\n",
    "                (forged_img, forged_mask)\n",
    "            ])\n",
    "        elif has_auth:\n",
    "            # Standalone authentic\n",
    "            singletons.append([(authentic_dict[stem], None)])\n",
    "        else:\n",
    "            # Standalone forged\n",
    "            forged_img = forged_dict[stem]\n",
    "            forged_mask = mask_dir / (stem + \".npy\")\n",
    "            singletons.append([(forged_img, forged_mask)])\n",
    "\n",
    "    print(f\"Found {len(paired_groups)} paired authentic/forged groups.\")\n",
    "    print(f\"Found {len(singletons)} singleton items.\")\n",
    "\n",
    "    # --- Split groups into train/val ---\n",
    "    random.seed(CFG.seed)\n",
    "    random.shuffle(paired_groups)\n",
    "    random.shuffle(singletons)\n",
    "\n",
    "    total_units = len(paired_groups) + len(singletons)\n",
    "    val_units = int(total_units * val_ratio)\n",
    "\n",
    "    # First fill val with some pairs (keep them intact)\n",
    "    val_groups = []\n",
    "    train_groups = []\n",
    "\n",
    "    # Add pairs to val until close to ratio\n",
    "    for group in paired_groups:\n",
    "        if len(val_groups) < val_units:\n",
    "            val_groups.append(group)\n",
    "        else:\n",
    "            train_groups.append(group)\n",
    "\n",
    "    # Add singletons\n",
    "    for group in singletons:\n",
    "        if len(val_groups) < val_units:\n",
    "            val_groups.append(group)\n",
    "        else:\n",
    "            train_groups.append(group)\n",
    "\n",
    "    # --- Flatten lists ---\n",
    "    train_images = []\n",
    "    train_masks  = []\n",
    "    val_images   = []\n",
    "    val_masks    = []\n",
    "\n",
    "    def append_group(groups, img_list, mask_list):\n",
    "        for group in groups:\n",
    "            for img_path, mask_path in group:\n",
    "                img_list.append(img_path)\n",
    "                mask_list.append(mask_path)\n",
    "\n",
    "    append_group(train_groups, train_images, train_masks)\n",
    "    append_group(val_groups,   val_images,  val_masks)\n",
    "\n",
    "    print(f\"Final split → train: {len(train_images)}, val: {len(val_images)}\")\n",
    "\n",
    "    # --- Build datasets ---\n",
    "    train_ds = CopyMoveDataset(train_images, train_masks, CFG.img_size)\n",
    "    val_ds   = CopyMoveDataset(val_images, val_masks, CFG.img_size)\n",
    "\n",
    "    # --- Loaders ---\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference From predicted maps\n",
    "\n",
    "Will create infered maps for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_postprocess(preds, original_size, alpha_grad=None, threshold_multiplier=None):\n",
    "    \"\"\"Enhanced post-processing with tuned parameters\"\"\" \n",
    "    if alpha_grad is None:\n",
    "        alpha_grad = CFG.alpha_grad \n",
    "    if threshold_multiplier is None:\n",
    "        threshold_multiplier = CFG.threshold_multiplier\n",
    "        \n",
    "    gx = cv2.Sobel(preds, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    gy = cv2.Sobel(preds, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
    "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
    "    enhanced = (1 - alpha_grad) * preds + alpha_grad * grad_norm\n",
    "    enhanced = cv2.GaussianBlur(enhanced, (3, 3), 0)\n",
    "    thr = np.mean(enhanced) + threshold_multiplier * np.std(enhanced)\n",
    "    mask = (enhanced > thr).astype(np.uint8)\n",
    "    # Apply morphological operations with tuned parameters \n",
    "    close_kernel = np.ones((CFG.morph_close_kernel, CFG.morph_close_kernel), np.uint8)\n",
    "    open_kernel = np.ones((CFG.morph_open_kernel, CFG.morph_open_kernel), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, close_kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, open_kernel)\n",
    "    mask = cv2.resize(mask, original_size, interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.T.flatten()\n",
    "    dots = np.where(pixels == 1)[0]\n",
    "    \n",
    "    if len(dots) == 0:\n",
    "        return \"authentic\"\n",
    "    \n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if b > prev + 1:\n",
    "            run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    \n",
    "    return json.dumps([int(x) for x in run_lengths])\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_tta(model, image):\n",
    "    predictions = []\n",
    "\n",
    "    # Original prediction\n",
    "    pred = torch.sigmoid(model.forward_seg(image))\n",
    "    predictions.append(pred)\n",
    "\n",
    "    # Horizontal flip TTA\n",
    "    if CFG.tta_flip_horizontal:\n",
    "        pred = torch.sigmoid(model.forward_seg(torch.flip(image, dims=[3])))\n",
    "        predictions.append(torch.flip(pred, dims=[3]))\n",
    "\n",
    "    # Vertical flip TTA\n",
    "    if CFG.tta_flip_vertical:\n",
    "        pred = torch.sigmoid(model.forward_seg(torch.flip(image, dims=[2])))\n",
    "        predictions.append(torch.flip(pred, dims=[2]))\n",
    "\n",
    "    return torch.stack(predictions).mean(0)[0, 0].cpu().numpy()\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, image):\n",
    "    return torch.sigmoid(model.forward_seg(image))[0,0].cpu().numpy()\n",
    "\n",
    "def infer_image(image, area_cutoff=400, mean_cutoff=0.3):\n",
    "    image_array = np.array(image.resize((CFG.img_size, CFG.img_size)), np.float32) / 255\n",
    "    image_array = torch.from_numpy(image_array).permute(2, 0, 1)[None].to(CFG.device)\n",
    "    \n",
    "    if CFG.use_tta:\n",
    "        preds = predict_with_tta(model, image_array)\n",
    "    else:\n",
    "        preds = predict(model, image_array)\n",
    "    \n",
    "    mask = enhanced_postprocess(preds, image.size)\n",
    "    \n",
    "    area = int(mask.sum())\n",
    "    if area > 0:\n",
    "        resized_mask = cv2.resize(mask, (CFG.img_size, CFG.img_size), interpolation=cv2.INTER_NEAREST)\n",
    "        mean_inside = float(preds[resized_mask == 1].mean())\n",
    "    else:\n",
    "        mean_inside = 0.0\n",
    "\n",
    "    # Enhanced decision logic with tuned thresholds\n",
    "    if area < area_cutoff or mean_inside < mean_cutoff:\n",
    "        return \"authentic\", None    \n",
    "    \n",
    "    return \"forged\", mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "def precompute_val_predictions(model, val_loader):\n",
    "    \"\"\"\n",
    "    Run the model once on all validation images and store the raw predictions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_data = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=\"Running model on validation set\"):\n",
    "            B = images.size(0)\n",
    "            for b in range(B):\n",
    "                # Image\n",
    "                image_tensor = images[b]\n",
    "                pil_img = Image.fromarray((image_tensor.permute(1,2,0).cpu().numpy() * 255).astype(np.uint8))\n",
    "\n",
    "                # Ground truth\n",
    "                gt_mask = masks[b,0].cpu().numpy()\n",
    "                gt_label = \"forged\" if gt_mask.sum() > 0 else \"authentic\"\n",
    "\n",
    "                # --- Forward pass ---\n",
    "                image_array = np.array(pil_img.resize((CFG.img_size, CFG.img_size)), np.float32) / 255\n",
    "                image_array = torch.from_numpy(image_array).permute(2,0,1)[None].to(CFG.device)\n",
    "\n",
    "                if CFG.use_tta:\n",
    "                    preds = predict_with_tta(model, image_array)\n",
    "                else:\n",
    "                    preds = predict(model, image_array)\n",
    "\n",
    "                val_data.append({\n",
    "                    \"pil_image\": pil_img,\n",
    "                    \"gt_mask\": gt_mask,\n",
    "                    \"gt_label\": gt_label,\n",
    "                    \"raw_pred\": preds  # float32 numpy array, resized to original size\n",
    "                })\n",
    "\n",
    "    print(f\"Precomputed predictions for {len(val_data)} validation images.\")\n",
    "    return val_data\n",
    "\n",
    "\n",
    "def fast_tune_infer_cutoffs(val_data, area_range, mean_range):\n",
    "    \"\"\"\n",
    "    Grid search using precomputed raw predictions\n",
    "    \"\"\"\n",
    "    best_f1 = -1\n",
    "    best_params = {}\n",
    "\n",
    "    for area_cut in area_range:\n",
    "        for mean_cut in mean_range:\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "\n",
    "            for item in val_data:\n",
    "                preds = item[\"raw_pred\"]\n",
    "                gt_label = item[\"gt_label\"]\n",
    "                gt_mask  = item[\"gt_mask\"]\n",
    "\n",
    "                # --- Apply post-processing and decision logic only ---\n",
    "                mask = enhanced_postprocess(preds, item[\"pil_image\"].size)\n",
    "                area = int(mask.sum())\n",
    "                if area > 0:\n",
    "                    resized_mask = cv2.resize(mask, (CFG.img_size, CFG.img_size), interpolation=cv2.INTER_NEAREST)\n",
    "                    mean_inside = float(preds[resized_mask == 1].mean())\n",
    "                else:\n",
    "                    mean_inside = 0.0\n",
    "\n",
    "                if area < area_cut or mean_inside < mean_cut:\n",
    "                    pred_label = \"authentic\"\n",
    "                else:\n",
    "                    pred_label = \"forged\"\n",
    "\n",
    "                y_true.append(gt_label)\n",
    "                y_pred.append(pred_label)\n",
    "\n",
    "            f1 = f1_score(y_true, y_pred, pos_label=\"forged\")\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_params = {\n",
    "                    \"area_cutoff\": area_cut,\n",
    "                    \"mean_cutoff\": mean_cut\n",
    "                }\n",
    "\n",
    "    print(f\"\\nBest thresholds: {best_params}, F1 Score: {best_f1:.4f}\")\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Metrics\n",
    "\n",
    "Displays first few actual maps, summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(pred_mask, gt_mask):\n",
    "    pred = pred_mask > 0\n",
    "    gt = gt_mask > 0\n",
    "    inter = np.logical_and(pred, gt).sum()\n",
    "    union = np.logical_or(pred, gt).sum()\n",
    "    if union == 0:\n",
    "        return 1.0 if inter == 0 else 0.0\n",
    "    return inter / union\n",
    "\n",
    "def compute_dice(pred_mask, gt_mask):\n",
    "    pred = pred_mask > 0\n",
    "    gt = gt_mask > 0\n",
    "    inter = np.logical_and(pred, gt).sum()\n",
    "    denom = pred.sum() + gt.sum()\n",
    "    if denom == 0:\n",
    "        return 1.0 if inter == 0 else 0.0\n",
    "    return 2 * inter / denom\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "#      VALIDATION PREDICTION PIPELINE USING infer_image()\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def evaluate_on_validation(model, val_loader, max_visualize=10, area_cutoff=400, mean_cutoff=0.3):\n",
    "\n",
    "    all_labels = []      # true labels (authentic/forged)\n",
    "    all_preds  = []      # predicted labels\n",
    "    iou_scores = []      # mask IoU for forged examples\n",
    "    dice_scores = []     # mask dice for forged examples\n",
    "\n",
    "    visualizations = []\n",
    "\n",
    "    count_visual = 0\n",
    "\n",
    "    for images, masks in val_loader:\n",
    "\n",
    "        B = images.size(0)\n",
    "\n",
    "        for b in range(B):\n",
    "\n",
    "            image_tensor = images[b]\n",
    "            mask_tensor = masks[b, 0]\n",
    "\n",
    "            # Convert back to PIL for infer_image()\n",
    "            image_np = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "            image_np = (image_np * 255).clip(0, 255).astype(np.uint8)\n",
    "            pil_img = Image.fromarray(image_np)\n",
    "\n",
    "            gt_mask = mask_tensor.cpu().numpy()\n",
    "\n",
    "            # --- infer_image (classification + mask prediction) ---\n",
    "            pred_label, pred_mask = infer_image(pil_img, area_cutoff=area_cutoff, mean_cutoff=mean_cutoff)\n",
    "\n",
    "            # GT label\n",
    "            true_label = \"forged\" if gt_mask.sum() > 0 else \"authentic\"\n",
    "\n",
    "            all_labels.append(true_label)\n",
    "            all_preds.append(pred_label)\n",
    "\n",
    "            # Compute mask metrics only for forged images\n",
    "            if true_label == \"forged\":\n",
    "                if pred_mask is None:\n",
    "                    pred_mask = np.zeros_like(gt_mask)\n",
    "\n",
    "                pred_mask = pred_mask.astype(np.uint8)\n",
    "                iou_scores.append(compute_iou(pred_mask, gt_mask))\n",
    "                dice_scores.append(compute_dice(pred_mask, gt_mask))\n",
    "\n",
    "            # Save visualizations\n",
    "            if count_visual < max_visualize:\n",
    "                visualizations.append((pil_img, gt_mask, pred_mask, true_label, pred_label))\n",
    "                count_visual += 1\n",
    "\n",
    "\n",
    "    # ---------------- Metrics ----------------\n",
    "    accuracy  = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, pos_label=\"forged\")\n",
    "    recall    = recall_score(all_labels, all_preds, pos_label=\"forged\")\n",
    "    f1        = f1_score(all_labels, all_preds, pos_label=\"forged\")\n",
    "    cm        = confusion_matrix(all_labels, all_preds, labels=[\"authentic\", \"forged\"])\n",
    "\n",
    "    print(\"\\n================ Validation Results ================\\n\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
    "    print(\"            authentic   forged\")\n",
    "    print(f\"authentic |   {cm[0,0]:5d}      {cm[0,1]:5d}\")\n",
    "    print(f\"forged    |   {cm[1,0]:5d}      {cm[1,1]:5d}\")\n",
    "\n",
    "    if iou_scores:\n",
    "        print(f\"\\nMean IoU (forged only):  {np.mean(iou_scores):.4f}\")\n",
    "        print(f\"Mean Dice (forged only): {np.mean(dice_scores):.4f}\")\n",
    "    else:\n",
    "        print(\"\\nNo forged samples found in validation set!\")\n",
    "\n",
    "\n",
    "    # ---------------- Visualization ----------------\n",
    "    print(\"\\nShowing first validation predictions...\\n\")\n",
    "\n",
    "    rows = len(visualizations)\n",
    "    plt.figure(figsize=(12, 4 * rows))\n",
    "\n",
    "    for i, (img, gt_mask, pred_mask, true_lbl, pred_lbl) in enumerate(visualizations):\n",
    "    \n",
    "        # Ensure pred_mask is a proper numeric array\n",
    "        if pred_mask is None:\n",
    "            pred_mask = np.zeros_like(gt_mask, dtype=np.uint8)\n",
    "    \n",
    "        # Original\n",
    "        plt.subplot(rows, 3, i*3 + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Image\\nLabel: {true_lbl}\")\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "        # Ground truth\n",
    "        plt.subplot(rows, 3, i*3 + 2)\n",
    "        plt.imshow(gt_mask, cmap=\"gray\")\n",
    "        plt.title(\"Ground Truth Mask\")\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "        # Predicted\n",
    "        plt.subplot(rows, 3, i*3 + 3)\n",
    "        plt.imshow(pred_mask, cmap=\"gray\")\n",
    "        plt.title(f\"Predicted Mask\\nPred Label: {pred_lbl}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Model Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intial Define and train\n",
    "train_loader, val_loader = get_train_val_loaders()\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(CFG.dino_path, local_files_only=True)\n",
    "encoder = AutoModel.from_pretrained(CFG.dino_path, local_files_only=True).eval().to(CFG.device)\n",
    "\n",
    "model = DinoSegmenter(encoder, processor).to(CFG.device)\n",
    "\n",
    "train_decoder(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=5,\n",
    "    lr=1e-4,\n",
    "    dice_weight=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search over cutoffs\n",
    "val_data = precompute_val_predictions(model, val_loader)\n",
    "\n",
    "# Step 2: Run fast grid search\n",
    "area_candidates = [100, 200, 300, 400, 500, 600]\n",
    "mean_candidates = [0.1, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "\n",
    "best_thresholds = fast_tune_infer_cutoffs(val_data, area_candidates, mean_candidates)\n",
    "\n",
    "print(f'Best Area Threshold: {best_thresholds[\"area_cutoff\"] }\\nBest Mean Threshold: {best_thresholds[\"mean_cutoff\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_on_validation(model, val_loader, area_cutoff=best_thresholds[\"area_cutoff\"], mean_cutoff=best_thresholds[\"mean_cutoff\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/kaggle/working/model_trained_5epochs_pair.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intial Define and train\n",
    "train_loader, val_loader = get_train_val_loaders()\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(CFG.dino_path, local_files_only=True)\n",
    "encoder = AutoModel.from_pretrained(CFG.dino_path, local_files_only=True).eval().to(CFG.device)\n",
    "\n",
    "model = DinoSegmenter(encoder, processor).to(CFG.device)\n",
    "\n",
    "state_dict = torch.load(\"/kaggle/working/model_trained_5epochs.pth\", map_location=CFG.device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_decoder(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=5,\n",
    "    lr=1e-4,\n",
    "    dice_weight=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search over cutoffs\n",
    "val_data = precompute_val_predictions(model, val_loader)\n",
    "\n",
    "# Step 2: Run fast grid search\n",
    "area_candidates = [100, 200, 300, 400, 500, 600]\n",
    "mean_candidates = [0.1, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "\n",
    "best_thresholds = fast_tune_infer_cutoffs(val_data, area_candidates, mean_candidates)\n",
    "\n",
    "print(f\"Best Area Threshold: {best_thresholds['area_cutoff']}\\nBest Mean Threshold: {best_thresholds['mean_cutoff']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_on_validation(model, val_loader, area_cutoff=best_thresholds[\"area_cutoff\"], mean_cutoff=best_thresholds[\"mean_cutoff\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/kaggle/working/model_trained_10epochs_pair.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_decoder(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=5,\n",
    "    lr=1e-4,\n",
    "    dice_weight=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search over cutoffs\n",
    "val_data = precompute_val_predictions(model, val_loader)\n",
    "\n",
    "# Step 2: Run fast grid search\n",
    "area_candidates = [100, 200, 300, 400, 500, 600]\n",
    "mean_candidates = [0.1, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "\n",
    "best_thresholds = fast_tune_infer_cutoffs(val_data, area_candidates, mean_candidates)\n",
    "\n",
    "print(f\"Best Area Threshold: {best_thresholds['area_cutoff']}\\nBest Mean Threshold: {best_thresholds['mean_cutoff']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_on_validation(model, val_loader, area_cutoff=best_thresholds[\"area_cutoff\"], mean_cutoff=best_thresholds[\"mean_cutoff\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/kaggle/working/model_trained_15epochs_pair.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN on predicted masks to class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierInputDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for training the mask classifier.\n",
    "    \n",
    "    It does NOT run the segmenter.\n",
    "    It only returns:\n",
    "        - raw image tensor (3×H×W)\n",
    "        - label: forged=1, authentic=0\n",
    "\n",
    "    The classifier model itself handles running the frozen segmenter.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_tensor, mask_tensor = self.base_dataset[idx]  # mask_tensor = GT mask\n",
    "\n",
    "        # label: forged = 1 if GT mask has any area\n",
    "        label = 1 if mask_tensor.sum() > 0 else 0\n",
    "\n",
    "        return image_tensor, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def get_classifier_loaders(train_dataset, val_dataset, batch_size=16):\n",
    "    train_ds = ClassifierInputDataset(train_dataset)\n",
    "    val_ds   = ClassifierInputDataset(val_dataset)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Classifier architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenSegmenterMaskClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "        raw RGB image → frozen segmenter → predicted mask → tiny CNN classifier (trainable)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, segmenter):\n",
    "        super().__init__()\n",
    "\n",
    "        # Store the frozen segmenter\n",
    "        self.segmenter = segmenter\n",
    "        self.segmenter.eval()\n",
    "        for p in self.segmenter.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Small CNN for classification on predicted mask\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=5, stride=4, padding=2),  # 512 → 128\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=4, padding=1), # 128 → 32\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # 32 → 16\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),  # (B,32,1,1)\n",
    "            nn.Flatten(),             # (B,32)\n",
    "            nn.Linear(32, 1)          # (B,1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x:\n",
    "            Either raw image tensor (B,3,512,512)\n",
    "            OR already-segmented mask tensor (B,1,512,512)\n",
    "\n",
    "        Returns:\n",
    "            logits (B,1)\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # 1. If input is 3-ch RGB → run the frozen segmenter\n",
    "        # ------------------------------------------------------\n",
    "        if x.shape[1] == 3:     # RGB input\n",
    "            with torch.no_grad():\n",
    "                pred_mask = torch.sigmoid(self.segmenter.forward_seg(x))\n",
    "        else:\n",
    "            # Already a mask\n",
    "            pred_mask = x\n",
    "\n",
    "        # Ensure mask size is (B,1,512,512)\n",
    "        if pred_mask.shape[1] != 1:\n",
    "            pred_mask = pred_mask[:, :1]\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # 2. Pass mask through the trainable classifier\n",
    "        # ------------------------------------------------------\n",
    "        feats = self.features(pred_mask)\n",
    "        out = self.classifier(feats)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mask_classifier(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=10,\n",
    "    lr=1e-4,\n",
    "    device=\"cuda\",\n",
    "    save_path=\"mask_classifier_best.pth\"\n",
    "):\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\n======== Epoch {epoch}/{num_epochs} ========\")\n",
    "\n",
    "        # -----------------------\n",
    "        # TRAIN\n",
    "        # -----------------------\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for masks, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            masks = masks.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            # Forward\n",
    "            logits = model(masks)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # Predictions\n",
    "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int).flatten()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "        train_loss = sum(train_losses) / len(train_losses)\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}  |  Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "        # -----------------------\n",
    "        # VALIDATION\n",
    "        # -----------------------\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_preds, val_labels_list = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for masks, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "                masks = masks.to(device)\n",
    "                labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "                logits = model(masks)\n",
    "                loss = criterion(logits, labels)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                preds = (probs > 0.5).astype(int).flatten()\n",
    "\n",
    "                val_preds.extend(preds)\n",
    "                val_labels_list.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "        val_loss = sum(val_losses) / len(val_losses)\n",
    "        val_acc = accuracy_score(val_labels_list, val_preds)\n",
    "        val_prec = precision_score(val_labels_list, val_preds, zero_division=0)\n",
    "        val_rec = recall_score(val_labels_list, val_preds, zero_division=0)\n",
    "        val_f1 = f1_score(val_labels_list, val_preds, zero_division=0)\n",
    "\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | \"\n",
    "              f\"Prec: {val_prec:.4f} | Rec: {val_rec:.4f} | F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"✔ Saved new best model (F1={best_val_f1:.4f})\")\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(f\"Best F1 Score: {best_val_f1:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of Post-Unet Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load your segmentation model\n",
    "state_dict = torch.load(\"/kaggle/working/model_trained_10epochs.pth\", map_location=CFG.device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "# 3. Create prediction mask loaders\n",
    "train_pred_loader, val_pred_loader = get_classifier_loaders(\n",
    "    train_loader.dataset,\n",
    "    val_loader.dataset,\n",
    "    batch_size=CFG.batch_size\n",
    ")\n",
    "\n",
    "post_model = FrozenSegmenterMaskClassifier(model)\n",
    "\n",
    "trained_model = train_mask_classifier(\n",
    "    post_model,\n",
    "    train_pred_loader,\n",
    "    val_pred_loader,\n",
    "    num_epochs=5,\n",
    "    lr=1e-4,\n",
    "    device=CFG.device,\n",
    "    save_path=\"/kaggle/working/tiny_mask_classifier_best.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14456136,
     "sourceId": 113558,
     "sourceType": "competition"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 504592,
     "modelInstanceId": 489174,
     "sourceId": 648498,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
